{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d71cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "909563f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels defined: {'O': 0, 'B-Object': 1, 'I-Object': 2, 'B-Aspect': 3, 'I-Aspect': 4, 'B-Predicate': 5, 'I-Predicate': 6}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Define the label mappings exactly as described in the PDF\n",
    "label_list = [\"O\", \"B-Object\", \"I-Object\", \"B-Aspect\", \"I-Aspect\", \"B-Predicate\", \"I-Predicate\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "print(f\"Labels defined: {label2id}\")\n",
    "\n",
    "# 2. Function to read the CoNLL/TSV file\n",
    "def read_conll_file(file_path, has_labels=True):\n",
    "    data = {'tokens': [], 'ner_tags': []}\n",
    "    \n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Empty line means end of sentence\n",
    "            if not line:\n",
    "                if current_tokens:\n",
    "                    data['tokens'].append(current_tokens)\n",
    "                    if has_labels:\n",
    "                        data['ner_tags'].append(current_labels)\n",
    "                    current_tokens = []\n",
    "                    current_labels = []\n",
    "                continue\n",
    "            \n",
    "            # Split line by tab\n",
    "            parts = line.split('\\t')\n",
    "            \n",
    "            # Safety check: ensure line has content\n",
    "            current_tokens.append(parts[0])\n",
    "            \n",
    "            if has_labels:\n",
    "                # If the file has labels, map the string label to its ID\n",
    "                # Default to 0 (\"O\") if something goes wrong\n",
    "                label_str = parts[1] if len(parts) > 1 else \"O\" \n",
    "                current_labels.append(label2id.get(label_str, 0))\n",
    "\n",
    "        # Catch the last sentence if there's no newline at the end\n",
    "        if current_tokens:\n",
    "            data['tokens'].append(current_tokens)\n",
    "            if has_labels:\n",
    "                data['ner_tags'].append(current_labels)\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "736c2304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Training sentences: 2100\n",
      "Validation sentences: 234\n",
      "\n",
      "Example sentence 0:\n",
      "['in', 'a', '2005', 'article', ',', 'curt', 'hibbs', 'famously', 'claimed', 'that', 'ruby', 'on', 'rails', 'gives', '10', 'times', 'greater', 'productivity', 'than', 'a', 'typical', 'java', 'framework', '.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 3, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 3. Load the data\n",
    "raw_data = read_conll_file(\"train.tsv\", has_labels=True)\n",
    "\n",
    "# 4. Split into Train (90%) and Validation (10%)\n",
    "# We split the lists of sentences\n",
    "train_tokens, val_tokens, train_labels, val_labels = train_test_split(\n",
    "    raw_data['tokens'], \n",
    "    raw_data['ner_tags'], \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5. Create Hugging Face Datasets\n",
    "train_dataset = Dataset.from_dict({'tokens': train_tokens, 'ner_tags': train_labels})\n",
    "valid_dataset = Dataset.from_dict({'tokens': val_tokens, 'ner_tags': val_labels})\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset\n",
    "})\n",
    "\n",
    "# 6. Verify it looks correct\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Training sentences: {len(dataset['train'])}\")\n",
    "print(f\"Validation sentences: {len(dataset['validation'])}\")\n",
    "print(\"\\nExample sentence 0:\")\n",
    "print(dataset['train'][0]['tokens'])\n",
    "print(dataset['train'][0]['ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "575439b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags_ids</th>\n",
       "      <th>ner_tags_str</th>\n",
       "      <th>sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[in, a, 2005, article, ,, curt, hibbs, famously, claimed, that, ruby, on, rails, gives, 10, times, greater, productivity, than, a, typical, java, framework, .]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 3, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-Object, O, O, O, O, O, B-Predicate, B-Aspect, O, O, O, B-Object, O, O]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[amazon, will, come, up, with, a, better, tablet, ,, but, the, quality, will, most, likely, never, match, ipad, ,, since, apple, can, afford, to, deliver, superior, quality, by, setting, price, points, at, much, higher, levels, .]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 3, 0, 0, 3, 0, 0, 0, 5, 0, 0]</td>\n",
       "      <td>[B-Object, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-Object, O, O, O, O, B-Predicate, B-Aspect, O, O, B-Aspect, O, O, O, B-Predicate, O, O]</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[microsoft, is, offering, verizon, better, terms, than, google, has, (, and, possibly, ever, will, ),, including, better, revenue, sharing, and, guarantees, of, higher, payments, .]</td>\n",
       "      <td>[1, 0, 0, 0, 5, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3, 0, 0, 0, 0, 5, 3, 0]</td>\n",
       "      <td>[B-Object, O, O, O, B-Predicate, B-Aspect, O, B-Object, O, O, O, O, O, O, O, O, B-Predicate, B-Aspect, O, O, O, O, B-Predicate, B-Aspect, O]</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[however, ,, the, golf, ball, is, much, heavier, than, the, table, tennis, ball, .]</td>\n",
       "      <td>[0, 0, 0, 1, 3, 0, 0, 5, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[O, O, O, B-Object, B-Aspect, O, O, B-Predicate, O, O, O, B-Object, O, O]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[these, figures, should, appear, at, least, a, little, bit, surprising, as, wii, and, ds, both, have, more, top, hit, software, than, ps2, even, though, the, ps2, hardware, base, was, bigger, than, either, platform, in, 2000, -, 2009, .]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-Object, O, B-Object, O, O, O, O, O, O, O, B-Object, O, O, O, B-Object, O, O, O, B-Predicate, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                          tokens  \\\n",
       "0                                                                                [in, a, 2005, article, ,, curt, hibbs, famously, claimed, that, ruby, on, rails, gives, 10, times, greater, productivity, than, a, typical, java, framework, .]   \n",
       "1         [amazon, will, come, up, with, a, better, tablet, ,, but, the, quality, will, most, likely, never, match, ipad, ,, since, apple, can, afford, to, deliver, superior, quality, by, setting, price, points, at, much, higher, levels, .]   \n",
       "2                                                          [microsoft, is, offering, verizon, better, terms, than, google, has, (, and, possibly, ever, will, ),, including, better, revenue, sharing, and, guarantees, of, higher, payments, .]   \n",
       "3                                                                                                                                                            [however, ,, the, golf, ball, is, much, heavier, than, the, table, tennis, ball, .]   \n",
       "4  [these, figures, should, appear, at, least, a, little, bit, surprising, as, wii, and, ds, both, have, more, top, hit, software, than, ps2, even, though, the, ps2, hardware, base, was, bigger, than, either, platform, in, 2000, -, 2009, .]   \n",
       "\n",
       "                                                                                                         ner_tags_ids  \\\n",
       "0                                            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 3, 0, 0, 0, 1, 0, 0]   \n",
       "1        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 3, 0, 0, 3, 0, 0, 0, 5, 0, 0]   \n",
       "2                                         [1, 0, 0, 0, 5, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3, 0, 0, 0, 0, 5, 3, 0]   \n",
       "3                                                                          [0, 0, 0, 1, 3, 0, 0, 5, 0, 0, 0, 1, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                                                                                   ner_tags_str  \\\n",
       "0                                                       [O, O, O, O, O, O, O, O, O, O, B-Object, O, O, O, O, O, B-Predicate, B-Aspect, O, O, O, B-Object, O, O]   \n",
       "1  [B-Object, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-Object, O, O, O, O, B-Predicate, B-Aspect, O, O, B-Aspect, O, O, O, B-Predicate, O, O]   \n",
       "2                  [B-Object, O, O, O, B-Predicate, B-Aspect, O, B-Object, O, O, O, O, O, O, O, O, B-Predicate, B-Aspect, O, O, O, O, B-Predicate, B-Aspect, O]   \n",
       "3                                                                                     [O, O, O, B-Object, B-Aspect, O, O, B-Predicate, O, O, O, B-Object, O, O]   \n",
       "4      [O, O, O, O, O, O, O, O, O, O, O, B-Object, O, B-Object, O, O, O, O, O, O, O, B-Object, O, O, O, B-Object, O, O, O, B-Predicate, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "   sentence_length  \n",
       "0               24  \n",
       "1               36  \n",
       "2               25  \n",
       "3               14  \n",
       "4               38  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Detailed View of Sentence #0 ---\n",
      "in                   O\n",
      "a                    O\n",
      "2005                 O\n",
      "article              O\n",
      ",                    O\n",
      "curt                 O\n",
      "hibbs                O\n",
      "famously             O\n",
      "claimed              O\n",
      "that                 O\n",
      "ruby                 B-Object\n",
      "on                   O\n",
      "rails                O\n",
      "gives                O\n",
      "10                   O\n",
      "times                O\n",
      "greater              B-Predicate\n",
      "productivity         B-Aspect\n",
      "than                 O\n",
      "a                    O\n",
      "typical              O\n",
      "java                 B-Object\n",
      "framework            O\n",
      ".                    O\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Create a DataFrame from the training lists\n",
    "df_train = pd.DataFrame({\n",
    "    'tokens': train_tokens,\n",
    "    'ner_tags_ids': train_labels\n",
    "})\n",
    "\n",
    "# 2. Add a column with human-readable labels (converting IDs 1 -> \"B-Object\")\n",
    "# We use the id2label dictionary defined in Step 1\n",
    "df_train['ner_tags_str'] = df_train['ner_tags_ids'].apply(\n",
    "    lambda ids: [id2label[i] for i in ids]\n",
    ")\n",
    "\n",
    "# 3. Add a column for sentence length (useful to see how long your inputs are)\n",
    "df_train['sentence_length'] = df_train['tokens'].apply(len)\n",
    "\n",
    "# 4. Show the top 5 rows\n",
    "pd.set_option('display.max_colwidth', None) # Show full text\n",
    "display(df_train.head())\n",
    "\n",
    "# --- OPTIONAL: Detailed View ---\n",
    "# If you want to see exactly how one specific sentence matches up word-for-word:\n",
    "print(\"\\n--- Detailed View of Sentence #0 ---\")\n",
    "sample_idx = 0\n",
    "for word, label in zip(df_train.iloc[sample_idx]['tokens'], df_train.iloc[sample_idx]['ner_tags_str']):\n",
    "    print(f\"{word:<20} {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f155c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Detailed View of Sentence #1 ---\n",
      "amazon               B-Object\n",
      "will                 O\n",
      "come                 O\n",
      "up                   O\n",
      "with                 O\n",
      "a                    O\n",
      "better               O\n",
      "tablet               O\n",
      ",                    O\n",
      "but                  O\n",
      "the                  O\n",
      "quality              O\n",
      "will                 O\n",
      "most                 O\n",
      "likely               O\n",
      "never                O\n",
      "match                O\n",
      "ipad                 O\n",
      ",                    O\n",
      "since                O\n",
      "apple                B-Object\n",
      "can                  O\n",
      "afford               O\n",
      "to                   O\n",
      "deliver              O\n",
      "superior             B-Predicate\n",
      "quality              B-Aspect\n",
      "by                   O\n",
      "setting              O\n",
      "price                B-Aspect\n",
      "points               O\n",
      "at                   O\n",
      "much                 O\n",
      "higher               B-Predicate\n",
      "levels               O\n",
      ".                    O\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Detailed View of Sentence #1 ---\")\n",
    "sample_idx = 1\n",
    "for word, label in zip(df_train.iloc[sample_idx]['tokens'], df_train.iloc[sample_idx]['ner_tags_str']):\n",
    "    print(f\"{word:<20} {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65cc4828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4372fc1782b3492880c99472c71f0927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7270699027824750b6ba635bc1bdb3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82de1be23a6b419fb06fe058777f2792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13b750c56214f3482e924d90a016e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782816304c1645d69079e4fffcc572b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d7e3d572fe49cda87f3b499928d347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ef837639934a7ea7bff1e5ac336acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/234 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens (<s>, </s>) map to None. Set to -100 to ignore in loss.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # If start of a new word, apply the label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # If it's a sub-word (continuation of previous word), ignore it\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply this function to the Hugging Face Dataset we created earlier\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91db38c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033689a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
